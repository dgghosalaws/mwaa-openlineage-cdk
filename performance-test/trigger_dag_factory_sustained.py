"""
Master Trigger DAG for DAG Factory Sustained Load Test

This DAG triggers all DAG Factory sustained test DAGs with wave-based timing.
Gradual ramp-up, sustained peak load, and gradual ramp-down.

Test Pattern (40 minutes total):
- 0-5 min: Ramp up from 500 → peak tasks
- 5-25 min: Sustain peak load (20 minutes)  
- 25-40 min: Ramp down peak → 0 tasks

Configuration:
- Generated by: generate_dag_factory_sustained.py --peak-tasks <N>
- Default: 2000 peak tasks, 20 minute peak duration

Usage:
1. Generate config: python generate_dag_factory_sustained.py --peak-tasks 2000
2. Upload files to S3
3. Wait for DAG parsing (2-5 minutes)
4. Trigger this DAG manually in Airflow UI
5. Monitor CloudWatch dashboard for sustained load metrics
"""

from airflow import DAG
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import logging
import yaml
import os

# Default arguments
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 0,
}

# Read config to determine number of DAGs and wave configuration
config_file = os.path.join(os.path.dirname(__file__), "dag_factory_config_sustained.yaml")
try:
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)
    num_dags = len(config)
    # Calculate total tasks from first DAG
    first_dag = list(config.values())[0]
    tasks_per_dag = len([t for t in first_dag['tasks'] if t['task_id'] != 'wave_delay'])
    total_tasks = num_dags * tasks_per_dag
except Exception as e:
    # Fallback to defaults if config can't be read
    num_dags = 65
    tasks_per_dag = 31
    total_tasks = 2015

# Create DAG
dag = DAG(
    'trigger_dag_factory_sustained_test',
    default_args=default_args,
    description=f'Master trigger for DAG Factory Sustained Load Test - {total_tasks} peak tasks',
    schedule=None,
    start_date=datetime(2024, 1, 1),
    catchup=False,
    max_active_runs=1,
    is_paused_upon_creation=False,
    tags=['dag-factory-test', 'sustained-load', 'master-trigger', 'performance', 'long-duration'],
)


def log_test_start(**context):
    """Log test start information"""
    logger = logging.getLogger(__name__)
    logger.info("=" * 80)
    logger.info("DAG FACTORY SUSTAINED LOAD TEST - STARTING")
    logger.info("=" * 80)
    logger.info(f"⚠️  SUSTAINED LOAD TEST - 40 MINUTE DURATION!")
    logger.info("")
    logger.info("Test Configuration:")
    logger.info(f"  - {num_dags} DAGs × {tasks_per_dag} tasks = {total_tasks} peak capacity")
    logger.info("  - Task duration: 20 minutes")
    logger.info("  - Peak sustained: 20 minutes")
    logger.info("  - Total test duration: ~40 minutes")
    logger.info("")
    logger.info("Test Pattern:")
    logger.info("  - 0-5 min: Ramp up from 500 → peak tasks")
    logger.info("  - 5-25 min: Sustain peak load (20 minutes)")
    logger.info("  - 25-40 min: Ramp down peak → 0 tasks")
    logger.info("")
    logger.info("Expected Behavior:")
    logger.info("  - Gradual worker scale-up during ramp")
    logger.info("  - Sustained high utilization for 20 minutes")
    logger.info("  - Gradual worker scale-down during ramp-down")
    logger.info("")
    logger.info(f"Triggering all {num_dags} DAGs with wave delays...")
    logger.info("=" * 80)
    return {'test_start_time': datetime.now().isoformat()}


def log_test_complete(**context):
    """Log test completion"""
    logger = logging.getLogger(__name__)
    logger.info("=" * 80)
    logger.info("DAG FACTORY SUSTAINED LOAD TEST - ALL DAGS TRIGGERED")
    logger.info("=" * 80)
    logger.info(f"All {num_dags} DAGs have been triggered with wave delays")
    logger.info("")
    logger.info("Next Steps:")
    logger.info("  1. Monitor CloudWatch dashboard for sustained load metrics")
    logger.info("  2. Watch worker auto-scaling behavior")
    logger.info(f"  3. Verify peak {total_tasks} tasks sustained for 20 minutes")
    logger.info("  4. Check individual DAG runs in Airflow UI")
    logger.info("  5. Wait ~40 minutes for complete test cycle")
    logger.info("")
    logger.info("Expected Timeline:")
    logger.info("  - T+0-5 min: Ramp up phase")
    logger.info("  - T+5-25 min: Peak sustained phase")
    logger.info("  - T+25-40 min: Ramp down phase")
    logger.info("=" * 80)
    return {'test_trigger_complete': datetime.now().isoformat()}


# Start task
start_task = PythonOperator(
    task_id='log_test_start',
    python_callable=log_test_start,
    dag=dag,
)

# Generate trigger tasks for all DAGs
# DAGs will be triggered with wave delays built into their configuration
trigger_tasks = []

for dag_num in range(num_dags):
    dag_id = f"factory_sustained_dag_{dag_num:03d}"
    
    trigger_task = TriggerDagRunOperator(
        task_id=f'trigger_{dag_id}',
        trigger_dag_id=dag_id,
        wait_for_completion=False,  # Don't wait, trigger all immediately
        dag=dag,
    )
    
    trigger_tasks.append(trigger_task)
    start_task >> trigger_task

# End task
end_task = PythonOperator(
    task_id='log_test_complete',
    python_callable=log_test_complete,
    dag=dag,
)

# Connect all trigger tasks to end task
for trigger_task in trigger_tasks:
    trigger_task >> end_task
